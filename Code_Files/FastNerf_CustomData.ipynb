{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameya1252/SeniorDesign/blob/main/fastnerfy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZvyjxlyxCPI",
        "outputId": "79647292-2624-4097-e9e1-1e416e8a128d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KOSuAkCkthnb",
        "outputId": "4085fa38-a86a-4f14-bee2-489c80edebd6"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function recordVideo(duration) {\n",
              "        // Create a video stream and a media recorder to record\n",
              "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "        let mediaRecorder = new MediaRecorder(stream);\n",
              "        let chunks = [];\n",
              "\n",
              "        // When data is available, push it to the chunks array\n",
              "        mediaRecorder.ondataavailable = event => chunks.push(event.data);\n",
              "\n",
              "        // Start recording\n",
              "        mediaRecorder.start();\n",
              "\n",
              "        // Create a video element to show the video stream\n",
              "        let video = document.createElement('video');\n",
              "        video.style.display = 'block';\n",
              "        video.srcObject = stream;\n",
              "        video.autoplay = true;\n",
              "        video.muted = true;\n",
              "        video.width = 224;\n",
              "        document.body.appendChild(video);\n",
              "\n",
              "        // Stop recording after the specified duration\n",
              "        await new Promise(resolve => setTimeout(resolve, duration * 1000));\n",
              "        mediaRecorder.stop();\n",
              "\n",
              "        // Remove the video element and stop the video stream\n",
              "        document.body.removeChild(video);\n",
              "        stream.getTracks().forEach(track => track.stop());\n",
              "\n",
              "        // Wait for the recorder to stop\n",
              "        await new Promise(resolve => mediaRecorder.onstop = resolve);\n",
              "\n",
              "        // Convert the chunks to a blob and create an object URL\n",
              "        const blob = new Blob(chunks, {type: 'video/mp4'});\n",
              "        const reader = new FileReader();\n",
              "\n",
              "        // Read the blob as base64 data and return it\n",
              "        reader.readAsDataURL(blob);\n",
              "        await new Promise(resolve => reader.onloadend = resolve);\n",
              "        return reader.result;\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def record_video(filename='video.mp4', duration=5):\n",
        "    js = Javascript(\"\"\"\n",
        "    async function recordVideo(duration) {\n",
        "        // Create a video stream and a media recorder to record\n",
        "        const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "        let mediaRecorder = new MediaRecorder(stream);\n",
        "        let chunks = [];\n",
        "\n",
        "        // When data is available, push it to the chunks array\n",
        "        mediaRecorder.ondataavailable = event => chunks.push(event.data);\n",
        "\n",
        "        // Start recording\n",
        "        mediaRecorder.start();\n",
        "\n",
        "        // Create a video element to show the video stream\n",
        "        let video = document.createElement('video');\n",
        "        video.style.display = 'block';\n",
        "        video.srcObject = stream;\n",
        "        video.autoplay = true;\n",
        "        video.muted = true;\n",
        "        video.width = 224;\n",
        "        document.body.appendChild(video);\n",
        "\n",
        "        // Stop recording after the specified duration\n",
        "        await new Promise(resolve => setTimeout(resolve, duration * 1000));\n",
        "        mediaRecorder.stop();\n",
        "\n",
        "        // Remove the video element and stop the video stream\n",
        "        document.body.removeChild(video);\n",
        "        stream.getTracks().forEach(track => track.stop());\n",
        "\n",
        "        // Wait for the recorder to stop\n",
        "        await new Promise(resolve => mediaRecorder.onstop = resolve);\n",
        "\n",
        "        // Convert the chunks to a blob and create an object URL\n",
        "        const blob = new Blob(chunks, {type: 'video/mp4'});\n",
        "        const reader = new FileReader();\n",
        "\n",
        "        // Read the blob as base64 data and return it\n",
        "        reader.readAsDataURL(blob);\n",
        "        await new Promise(resolve => reader.onloadend = resolve);\n",
        "        return reader.result;\n",
        "    }\n",
        "    \"\"\")\n",
        "    display(js)\n",
        "    video_base64 = eval_js('recordVideo({})'.format(duration))\n",
        "    video_data = base64.b64decode(video_base64.split(',')[1])\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(video_data)\n",
        "    return filename\n",
        "\n",
        "# Specify the duration of the recording in seconds\n",
        "duration = 5  # Change this to your desired recording length\n",
        "video_filename = record_video(duration=duration)\n",
        "\n",
        "# After recording, save the video to your Google Drive\n",
        "!cp \"{video_filename}\" \"/content/drive/My Drive/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1yMd8YnXhnI"
      },
      "outputs": [],
      "source": [
        "#using existing dataset for demo\n",
        "#video_filename = 'vid2.mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FepYE2an2guL",
        "outputId": "31c9efc7-b456-44ac-e28b-275f99350600"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vid2.mp4\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (4.8.0.76)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python) (1.25.2)\n",
            "Dataset saved to tiny_nerf_data.npz with 100 images.\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Specify the path to your video file on Google Drive\n",
        "video_path = '/content/drive/My Drive/'+video_filename  # Update this path\n",
        "print(video_filename)\n",
        "# Step 3: Install opencv-python if not already installed\n",
        "!pip install opencv-python\n",
        "\n",
        "# Import necessary libraries\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "\n",
        "# Function to extract frames\n",
        "def extract_frames(video_path, num_frames=100):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    interval = frame_count // num_frames\n",
        "\n",
        "    os.makedirs('ExtractedFrames', exist_ok=True)\n",
        "    count = 0\n",
        "    frame_idx = 0\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        if count % interval == 0 and frame_idx < num_frames:\n",
        "            frame_path = os.path.join('ExtractedFrames', f'frame_{frame_idx:03d}.jpg')\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            frame_idx += 1\n",
        "        count += 1\n",
        "        if frame_idx >= num_frames:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "# Extract frames from the video\n",
        "extract_frames(video_path)\n",
        "\n",
        "# Continue with your existing process\n",
        "image_directory = 'ExtractedFrames'  # Directory where frames are saved\n",
        "output_file = 'tiny_nerf_data.npz'  # Output file name\n",
        "H, W = 120, 156  # Desired image height and width, adjust as needed\n",
        "focal_length = 112  # Example focal length, adjust based on your needs\n",
        "\n",
        "# Load images\n",
        "image_files = sorted(glob.glob(f'{image_directory}/*.jpg'))\n",
        "images = []\n",
        "for file in image_files:\n",
        "    img = Image.open(file).convert('RGB')\n",
        "    img = img.resize((W, H))\n",
        "    images.append(np.array(img))\n",
        "images = np.array(images, dtype=np.float32) / 255.0  # Normalize images\n",
        "\n",
        "# Example poses (placeholder, replace with actual poses data)\n",
        "poses = np.zeros((len(images), 4, 4), dtype=np.float32)\n",
        "for i in range(len(images)):\n",
        "    poses[i] = np.eye(4, dtype=np.float32)\n",
        "\n",
        "# Save to .npz file\n",
        "np.savez(output_file, images=images, poses=poses, focal=focal_length)\n",
        "\n",
        "print(f'Dataset saved to {output_file} with {len(images)} images.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30-Dc9x7_w7",
        "outputId": "05059426-e817-4506-a936-f219e6cf9b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    %tensorflow_version 2.x\n",
        "\n",
        "import os, sys\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3EfWSuJ8C3U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you've already loaded your dataset into 'data'\n",
        "data = np.load('tiny_nerf_data.npz')\n",
        "images = data['images']\n",
        "poses = data['poses']\n",
        "focal = data['focal']\n",
        "\n",
        "# Determine image and pose dimensions\n",
        "H, W = images.shape[1:3]\n",
        "print(images.shape, poses.shape, focal)\n",
        "\n",
        "# Adjustment for test image and pose selection\n",
        "testimg, testpose = images[-50], poses[-50]  # Use the last image as test image\n",
        "images = images[:99,...,:3]  # Use the first 99 images for training\n",
        "poses = poses[:99]  # Corresponding poses for the training images\n",
        "\n",
        "# Display test image\n",
        "plt.imshow(testimg)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo8Dm-nB8IQ3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def posenc(x):\n",
        "    rets = [x]\n",
        "    for i in range(L_embed):\n",
        "        for fn in [tf.sin, tf.cos]:\n",
        "            rets.append(fn(2.**i * x))\n",
        "    return tf.concat(rets, -1)\n",
        "\n",
        "L_embed = 6\n",
        "embed_fn = posenc\n",
        "\n",
        "def init_model(D=8, W=256, L=6):\n",
        "    relu = tf.keras.layers.ReLU()\n",
        "    dense = lambda W=W, act=relu: tf.keras.layers.Dense(W, activation=act)\n",
        "\n",
        "    inputs = tf.keras.Input(shape=(3 + 3 * 2 * L))\n",
        "    outputs = inputs\n",
        "    for i in range(D):\n",
        "        outputs = dense()(outputs)\n",
        "        if i % 4 == 0 and i > 0:\n",
        "            outputs = tf.concat([outputs, inputs], -1)\n",
        "    outputs = dense(4, act=None)(outputs)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_rays(H, W, focal, c2w):\n",
        "    i, j = tf.meshgrid(tf.range(W, dtype=tf.float32), tf.range(H, dtype=tf.float32), indexing='xy')\n",
        "    dirs = tf.stack([(i - W * .5) / focal, -(j - H * .5) / focal, -tf.ones_like(i)], -1)\n",
        "    rays_d = tf.reduce_sum(dirs[..., np.newaxis, :] * c2w[:3, :3], -1)\n",
        "    rays_o = tf.broadcast_to(c2w[:3, -1], tf.shape(rays_d))\n",
        "    return rays_o, rays_d\n",
        "\n",
        "def render_rays(network_fn, rays_o, rays_d, near, far, N_samples, rand=False):\n",
        "\n",
        "    def batchify(fn, chunk=1024 * 32):\n",
        "        return lambda inputs: tf.concat([fn(inputs[i:i + chunk]) for i in range(0, inputs.shape[0], chunk)], 0)\n",
        "\n",
        "    # Compute 3D query points\n",
        "    z_vals = tf.linspace(near, far, N_samples)\n",
        "    if rand:\n",
        "        z_vals += tf.random.uniform(list(rays_o.shape[:-1]) + [N_samples]) * (far - near) / N_samples\n",
        "    pts = rays_o[..., None, :] + rays_d[..., None, :] * z_vals[..., :, None]\n",
        "\n",
        "    # Run network\n",
        "    pts_flat = tf.reshape(pts, [-1, 3])\n",
        "    pts_flat = embed_fn(pts_flat)\n",
        "    raw = batchify(network_fn)(pts_flat)\n",
        "    raw = tf.reshape(raw, list(pts.shape[:-1]) + [4])\n",
        "\n",
        "    # Compute opacities and colors\n",
        "    sigma_a = tf.nn.relu(raw[..., 3])\n",
        "    rgb = tf.math.sigmoid(raw[..., :3])\n",
        "\n",
        "    # Do volume rendering\n",
        "    dists = tf.concat([z_vals[..., 1:] - z_vals[..., :-1], tf.broadcast_to([1e10], z_vals[..., :1].shape)], -1)\n",
        "    alpha = 1. - tf.exp(-sigma_a * dists)\n",
        "    weights = alpha * tf.math.cumprod(1. - alpha + 1e-10, -1, exclusive=True)\n",
        "\n",
        "    rgb_map = tf.reduce_sum(weights[..., None] * rgb, -2)\n",
        "    depth_map = tf.reduce_sum(weights * z_vals, -1)\n",
        "    acc_map = tf.reduce_sum(weights, -1)\n",
        "\n",
        "    return rgb_map, depth_map, acc_map\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6NJyRgtW8Lqm"
      },
      "outputs": [],
      "source": [
        "model = init_model()\n",
        "##initial_learning_rate = 5e-4\n",
        "##lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "  ##  initial_learning_rate, decay_steps=100, decay_rate=0.9, staircase=True)\n",
        "\n",
        "##optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
        "optimizer = tf.keras.optimizers.Adam(5e-4)\n",
        "\n",
        "near = 5.\n",
        "far = 6.\n",
        "\n",
        "\n",
        "N_samples = 64\n",
        "N_iters = 1000\n",
        "psnrs = []\n",
        "iternums = []\n",
        "i_plot = 25\n",
        "\n",
        "import time\n",
        "t = time.time()\n",
        "for i in range(N_iters+1):\n",
        "\n",
        "    img_i = np.random.randint(images.shape[0])\n",
        "    target = images[img_i]\n",
        "    pose = poses[img_i]\n",
        "    rays_o, rays_d = get_rays(H, W, focal, pose)\n",
        "    with tf.GradientTape() as tape:\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=near, far=far, N_samples=N_samples, rand=True)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - target))\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    if i%i_plot==0:\n",
        "        print(i, (time.time() - t) / i_plot, 'secs per iter')\n",
        "        t = time.time()\n",
        "\n",
        "        # Render the holdout view for logging\n",
        "        rays_o, rays_d = get_rays(H, W, focal, testpose)\n",
        "        rgb, depth, acc = render_rays(model, rays_o, rays_d, near=2., far=6., N_samples=N_samples)\n",
        "        loss = tf.reduce_mean(tf.square(rgb - testimg))\n",
        "        psnr = -10. * tf.math.log(loss) / tf.math.log(10.)\n",
        "\n",
        "        psnrs.append(psnr.numpy())\n",
        "        iternums.append(i)\n",
        "\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.subplot(121)\n",
        "        plt.imshow(rgb)\n",
        "        plt.title(f'Iteration: {i}')\n",
        "        plt.subplot(122)\n",
        "        plt.plot(iternums, psnrs)\n",
        "        plt.title('PSNR')\n",
        "        plt.show()\n",
        "\n",
        "print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "--m-ZIOx8bt-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from ipywidgets import interactive, widgets\n",
        "\n",
        "\n",
        "trans_t = lambda t : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,1,0,0],\n",
        "    [0,0,1,t],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_phi = lambda phi : tf.convert_to_tensor([\n",
        "    [1,0,0,0],\n",
        "    [0,tf.cos(phi),-tf.sin(phi),0],\n",
        "    [0,tf.sin(phi), tf.cos(phi),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "rot_theta = lambda th : tf.convert_to_tensor([\n",
        "    [tf.cos(th),0,-tf.sin(th),0],\n",
        "    [0,1,0,0],\n",
        "    [tf.sin(th),0, tf.cos(th),0],\n",
        "    [0,0,0,1],\n",
        "], dtype=tf.float32)\n",
        "\n",
        "\n",
        "def pose_spherical(theta, phi, radius):\n",
        "    c2w = trans_t(radius)\n",
        "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
        "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
        "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
        "    return c2w\n",
        "\n",
        "def f(**kwargs):\n",
        "    c2w = pose_spherical(**kwargs)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=near, far=far, N_samples=N_samples)\n",
        "    img = np.clip(rgb,0,1)\n",
        "\n",
        "    plt.figure(2, figsize=(20,6))\n",
        "    plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
        "    value=v,\n",
        "    min=mi,\n",
        "    max=ma,\n",
        "    step=.01,\n",
        ")\n",
        "\n",
        "names = [\n",
        "    ['theta', [100., 0., 360]],\n",
        "    ['phi', [-30., -90, 0]],\n",
        "    ['radius', [4., 3., 5.]],\n",
        "]\n",
        "\n",
        "interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n",
        "output = interactive_plot.children[-1]\n",
        "output.layout.height = '350px'\n",
        "interactive_plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "32a18e55721e4b629a597c27456a9951"
          ]
        },
        "id": "VXlSBmwG8eX_",
        "outputId": "1c8a7fce-11fb-41a1-c366-0d2122dae421"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-a1c3c79e7e61>:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32a18e55721e4b629a597c27456a9951",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/120 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (156, 120) to (160, 128) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n"
          ]
        }
      ],
      "source": [
        "frames = []\n",
        "for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
        "    c2w = pose_spherical(th, -30., 4.)\n",
        "    rays_o, rays_d = get_rays(H, W, focal, c2w[:3,:4])\n",
        "    rgb, depth, acc = render_rays(model, rays_o, rays_d, near=near, far=far, N_samples=N_samples)\n",
        "    frames.append((255*np.clip(rgb,0,1)).astype(np.uint8))\n",
        "\n",
        "import imageio\n",
        "f = 'video.mp4'\n",
        "imageio.mimwrite(f, frames, fps=30, quality=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KOQ8auma8ggc"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls autoplay loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OfHWA3SHEVi_"
      },
      "outputs": [],
      "source": [
        "!cp video.mp4 \"/content/drive/My Drive/video.mp4\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": [],
      "authorship_tag": "ABX9TyMmUy4GnjLao/7vdzAjF+J0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
